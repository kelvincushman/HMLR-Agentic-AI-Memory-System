
"""
Single-step executor: turns a PageContext + goal into a CommandBatch via LLM,
executes it through BrowserController, and returns Evidence. Keeps concerns small:
- No Playwright primitives here (delegated to BrowserController)
- No lattice writes here (the orchestrator does logging + lattice updates)
- No DOM parsing here (dom_processor already built PageContext)

You can swap the LLM client as long as it exposes `await client.query_json(prompt: str) -> dict`.
"""
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
import json
import asyncio
from datetime import datetime

from .models import PageContext, CommandBatch, Command, Evidence

# Optional: import prompt_builder if present; fall back to local minimal builder
try:
    from .prompt_builder import build_reasoning_prompt
except Exception:  # pragma: no cover
    def build_reasoning_prompt(goal: str, ctx: PageContext, recent_actions: Optional[List[Dict[str, Any]]] = None) -> str:
        recent_actions = recent_actions or []
        skeleton = (ctx.skeleton or "")[:4000]
        # compact candidates
        candidates = []
        for el in ctx.interactive[:30]:
            candidates.append({
                "tag": el.tag,
                "text": el.text,
                "score": round(getattr(el, "score", 0.0), 3),
                "selectors": (el.selectors or [])[:3],
            })
        lines: List[str] = []
        lines.append("System:\n"
                     "You are a web-navigation planner. Given a goal, a DOM skeleton, and ranked candidates, "
                     "return 1-3 JSON commands that advance the goal. Prefer provided selectors.")
        lines.append("--- Goal ---\n" + goal.strip())
        lines.append(f"--- Page ---\nURL: {ctx.url}\nTitle: {ctx.title}\nSignature: {ctx.signature}")
        lines.append("--- DOM Skeleton (truncated) ---\n" + skeleton)
        lines.append("--- Ranked Candidates ---")
        for i, c in enumerate(candidates, 1):
            sels = ", ".join(c["selectors"]) if c["selectors"] else ""
            text = c["text"] or ""
            lines.append(f"{i}. <{c['tag']}> score={c['score']} text=\"{text}\" selectors=[{sels}]")
        if recent_actions:
            lines.append("--- Recent Actions ---")
            for a in recent_actions[-5:]:
                lines.append(f"- {a}")
        lines.append("--- Respond ---\n"
                     "Return JSON: {\"commands\":[], \"confidence\":0..1, \"rationale\":str}. Limit to 1‚Äì3 commands.")
        return "\n\n".join(lines)


@dataclass
class StepOutcome:
    batch: CommandBatch
    evidence: Evidence
    confidence: float
    rationale: str
    breadcrumb: str = ""  # Plain English note about what was accomplished


class StepExecutor:
    """Reason over the current page, produce a plan (CommandBatch), execute, return Evidence."""

    def __init__(self, browser_controller, llm_client, safety_manager=None, logger=None, debug_run_folder=None):
        self.browser = browser_controller
        self.llm = llm_client
        self.safety = safety_manager
        self.logger = logger
        self.debug_run_folder = debug_run_folder

    async def reason_and_act(
        self,
        goal: str,
        ctx: PageContext,
        mode: str = "autonomous",
        recent_actions: Optional[List[Dict[str, Any]]] = None,
        breadcrumbs: Optional[List[str]] = None,
    ) -> StepOutcome:
        """
        1) Build prompt from PageContext with progressive candidate disclosure
        2) Ask LLM for next 1‚Äì3 commands (JSON)
        3) If LLM says "NONE", increase candidates and retry
        4) Safety pre-check (if provided)
        5) Execute batch via BrowserController
        6) Return Evidence (+ confidence/rationale)
        """
        
        # Progressive candidate disclosure: start with 10, expand if needed
        max_passes = 5  # Up to 50 candidates (5 x 10)
        raw_response = None  # Initialize for the loop
        
        for pass_number in range(1, max_passes + 1):
            print(f"üîç Candidate pass {pass_number} (showing top {pass_number * 10} candidates)")
            
            # 1) Build prompt with current pass number
            prompt = build_reasoning_prompt(goal, ctx, recent_actions or [], breadcrumbs or [], pass_number)
            
            # ############################################################################# 
            # DEBUG: Save prompt to file for troubleshooting
            # ############################################################################# 
            # TODO: REMOVE OR COMMENT OUT ALL DEBUG CODE BELOW BEFORE PRODUCTION
            # ############################################################################# 
            try:
                import os
                debug_dir = self.debug_run_folder if self.debug_run_folder else os.path.join(os.getcwd(), "debug_prompts")
                os.makedirs(debug_dir, exist_ok=True)
                
                from datetime import datetime
                # Create unique filename with step info and microsecond precision
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                step_info = getattr(ctx, 'step_number', getattr(ctx, 'step', 'unknown'))
                total_info = getattr(ctx, 'total_steps', 'unknown')
                debug_file = os.path.join(debug_dir, f"web_prompt_step{step_info}of{total_info}_pass{pass_number}_{timestamp}.txt")
                
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write("################################################################################\n")
                    f.write(f"FULL PROMPT SENT TO EXTERNAL API - {datetime.now()}\n")
                    f.write("################################################################################\n")
                    f.write(f"Step: {step_info} of {total_info}\n")
                    f.write(f"Pass: {pass_number} (candidates: {pass_number * 10})\n")
                    f.write(f"Goal: {goal}\n")
                    f.write(f"URL: {ctx.url}\n")
                    f.write(f"Page Title: {ctx.title}\n")
                    f.write(f"Page Signature: {ctx.signature}\n")
                    f.write(f"Overall Goal: {getattr(ctx, 'overall_goal', 'not specified')}\n")
                    f.write("=" * 80 + "\n")
                    f.write("FULL PROMPT CONTENT:\n")
                    f.write("=" * 80 + "\n")
                    f.write(prompt)
                    f.write("\n" + "=" * 80 + "\n")
                    f.write(f"Prompt length: {len(prompt)} characters\n")
                    f.write("################################################################################\n")
                
                print(f"üêõ DEBUG: Prompt saved to {os.path.abspath(debug_file)}")
            except Exception as debug_error:
                print(f"‚ö†Ô∏è DEBUG: Failed to save prompt: {debug_error}")
            # ############################################################################# 
            
            # 2) Ask LLM with API retry logic
            api_retry_count = 0
            max_api_retries = 3
            
            while api_retry_count < max_api_retries:
                try:
                    raw_response = self.llm.query_external_api(prompt)
                    
                    # Check for connection error patterns in response
                    if ("I apologize, but I'm having trouble connecting" in raw_response or 
                        "Connection a" in raw_response or 
                        "Error: ('Connection" in raw_response or
                        "Could not parse JSON from response: I apologize" in raw_response):
                        api_retry_count += 1
                        print(f"üîÑ API connection error detected (retry {api_retry_count}/{max_api_retries})")
                        if api_retry_count < max_api_retries:
                            await asyncio.sleep(2)  # Wait before retry
                            continue
                        else:
                            print(f"‚ùå API connection failed after {max_api_retries} retries")
                            # Continue with failure response to handle gracefully below
                    
                    # Check if LLM said "NONE" (no suitable candidates)
                    if '"no_match"' in raw_response and '"NONE"' in raw_response:
                        print(f"üîÑ Pass {pass_number}: LLM found no suitable candidates, expanding to pass {pass_number + 1}")
                        if pass_number == max_passes:
                            print(f"‚ö†Ô∏è Reached maximum passes ({max_passes}), proceeding with best effort")
                            # Continue with the "NONE" response to handle gracefully
                            break
                        break  # Break from API retry loop to continue to next pass
                    
                    # LLM found a candidate, proceed with execution
                    print(f"‚úÖ Pass {pass_number}: LLM selected a candidate")
                    break  # Success, exit both loops
                    
                except Exception as llm_error:
                    api_retry_count += 1
                    print(f"‚ùå Pass {pass_number}: LLM query failed (retry {api_retry_count}/{max_api_retries}): {llm_error}")
                    if api_retry_count < max_api_retries:
                        await asyncio.sleep(2)  # Wait before retry
                        continue
                    else:
                        print(f"‚ùå API failed after {max_api_retries} retries")
                        if pass_number == max_passes:
                            raise  # Re-raise on final pass
                        break  # Move to next pass
            
            # If we successfully got a response (even if it's "NONE"), break from pass loop
            if api_retry_count < max_api_retries or '"no_match"' in raw_response:
                if '"no_match"' not in raw_response:
                    break  # Success case
                elif pass_number < max_passes:
                    continue  # "NONE" case, try next pass
                else:
                    break  # "NONE" on final pass
        
        # Continue with normal execution flow using the final raw_response
        # Handle case where all passes failed
        if raw_response is None:
            raw_response = '{"commands": [{"type": "noop"}], "confidence": 0.1, "rationale": "All candidate passes failed"}'
        
        # ############################################################################# 
        # DEBUG: Save LLM response to file for troubleshooting
        # ############################################################################# 
        # TODO: REMOVE OR COMMENT OUT THIS DEBUG CODE LATER
        # ############################################################################# 
        try:
            # Use same timestamp and step info for matching prompt/response pairs
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            step_info = getattr(ctx, 'step_number', getattr(ctx, 'step', 'unknown'))
            total_info = getattr(ctx, 'total_steps', 'unknown')
            debug_file = os.path.join(debug_dir, f"web_response_step{step_info}of{total_info}_{timestamp}.txt")
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write("################################################################################\n")
                f.write(f"FULL RESPONSE FROM EXTERNAL API - {datetime.now()}\n")
                f.write("################################################################################\n")
                f.write(f"Step: {step_info} of {total_info}\n")
                f.write(f"Goal: {goal}\n")
                f.write(f"URL: {ctx.url}\n")
                f.write(f"Overall Goal: {getattr(ctx, 'overall_goal', 'not specified')}\n")
                f.write("=" * 80 + "\n")
                f.write("RAW LLM RESPONSE CONTENT:\n")
                f.write("=" * 80 + "\n")
                f.write(raw_response)
                f.write("\n" + "=" * 80 + "\n")
                f.write(f"Response length: {len(raw_response)} characters\n")
                f.write("################################################################################\n")
            
            print(f"üêõ DEBUG: Response saved to {os.path.abspath(debug_file)}")
        except Exception as debug_error:
            print(f"‚ö†Ô∏è DEBUG: Failed to save response: {debug_error}")
            # ############################################################################# 
            # END DEBUG CODE - REMOVE BEFORE PRODUCTION
            # ############################################################################# 
        
        print(f"ü§ñ LLM Raw Response: {raw_response[:200]}...")  # Debug output
        
        # Try to extract JSON from response (in case there's extra text)
        # But first check if this is an API connection failure that needs retry
        retry_needed = (
            "I apologize, but I'm having trouble connecting" in raw_response or 
            "Connection a" in raw_response or 
            "Error: ('Connection" in raw_response
        )
        
        if retry_needed:
            print("üîÑ API connection failure detected in response, attempting retries...")
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                retry_count += 1
                print(f"üîÑ Retry attempt {retry_count}/{max_retries}")
                
                try:
                    await asyncio.sleep(2)  # Wait before retry
                    raw_response = self.llm.query_external_api(prompt)
                    
                    # Check if retry succeeded
                    if not ("I apologize, but I'm having trouble connecting" in raw_response or 
                           "Connection a" in raw_response or 
                           "Error: ('Connection" in raw_response):
                        print(f"‚úÖ Retry {retry_count} succeeded!")
                        break
                    else:
                        print(f"‚ùå Retry {retry_count} still has connection issues")
                        
                except Exception as retry_error:
                    print(f"‚ùå Retry {retry_count} failed: {retry_error}")
                    
                if retry_count == max_retries:
                    print(f"‚ùå All {max_retries} retries failed, proceeding with fallback")
        
        try:
            json_start = raw_response.find('{')
            json_end = raw_response.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = raw_response[json_start:json_end]
                raw = json.loads(json_str)
            else:
                # No JSON found, create fallback
                raw = {
                    "commands": [{"type": "noop"}], 
                    "confidence": 0.3, 
                    "rationale": f"Could not parse JSON from response: {raw_response[:100]}..."
                }
                
        except (AttributeError, json.JSONDecodeError, Exception) as e:
            # Fallback if API doesn't return valid JSON
            print(f"‚ùå JSON parsing failed: {e}")
            raw = {
                "commands": [{"type": "noop"}], 
                "confidence": 0.1, 
                "rationale": f"API error: {str(e)}"
            }
        
        commands, confidence, rationale, breadcrumb = self._parse_llm_json(raw)
        batch = CommandBatch(commands=commands)

        # Safety pre-check (optional)
        if self.safety is not None:
            decision = self.safety.requires_human_confirmation(
                command_batch=batch,
                ctx=ctx,
                mode=mode,
                confidence=confidence,
            )
            if getattr(decision, "require_confirmation", False):
                # Return a no-op Evidence; orchestrator can pause & ask user
                empty_ev = Evidence(
                    success=False,
                    dom_after_sig=ctx.signature,
                    regions_after=[],
                    findings={"pause_reasons": getattr(decision, "reasons", [])},
                    used_selector=None,
                    fallback_used=False,
                    timing_ms=0,
                    errors=[],
                )
                return StepOutcome(batch=batch, evidence=empty_ev, confidence=confidence, rationale=rationale, breadcrumb=breadcrumb)

        # 4) Execute batch with fallback logic
        evidence = await self._execute_with_fallbacks(batch, goal, ctx, recent_actions)

        # Log decision/result if logger provided
        if self.logger is not None:
            try:
                self.logger.log_decision(ctx, batch, mode, rationale, confidence)
                self.logger.log_result(ctx, batch, evidence)
            except Exception:
                pass

        return StepOutcome(batch=batch, evidence=evidence, confidence=confidence, rationale=rationale, breadcrumb=breadcrumb)

    async def _execute_with_fallbacks(
        self, 
        initial_batch: CommandBatch, 
        goal: str, 
        ctx: PageContext, 
        recent_actions: Optional[List[Dict[str, Any]]] = None
    ) -> Evidence:
        """
        Execute commands with fallback logic:
        1. Try initial batch (ranked interactive elements)
        2. If fails, retry with full DOM skeleton
        3. If still fails, retry with DOM diff context
        """
        # First attempt: Execute original batch
        evidence = await self.browser.execute_action_batch(initial_batch)
        
        # Check if we should try fallbacks:
        # 1. If there are errors (selectors not found, etc.)
        # 2. If interaction commands executed but DOM didn't change (suspicious)
        # 3. If success is False
        should_fallback = (
            len(evidence.errors) > 0 or 
            not evidence.success or
            (len(initial_batch.commands) > 0 and 
             any(cmd.type in ['click', 'type'] for cmd in initial_batch.commands) and
             not evidence.success)
        )
        
        # If successful, return immediately
        if not should_fallback:
            return evidence
            
        print(f"üîÑ First attempt needs fallback. Success: {evidence.success}, Errors: {len(evidence.errors)}")
        print(f"üîÑ Error details: {evidence.errors}")
        
        # Second attempt: Use full DOM skeleton for richer context
        try:
            fallback_batch = await self._create_fallback_batch_with_full_dom(goal, ctx, recent_actions)
            if fallback_batch and len(fallback_batch.commands) > 0:
                print("üîÑ Attempting fallback with full DOM skeleton...")
                fallback_evidence = await self.browser.execute_action_batch(fallback_batch)
                fallback_evidence.fallback_used = True
                
                # If this worked, return it
                if fallback_evidence.success or len(fallback_evidence.errors) == 0:
                    print("‚úÖ Fallback with full DOM succeeded!")
                    return fallback_evidence
                    
                print(f"üîÑ Full DOM fallback also failed with {len(fallback_evidence.errors)} errors.")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to create full DOM fallback: {e}")
        
        # Third attempt: Use DOM diff if available
        try:
            diff_batch = await self._create_fallback_batch_with_dom_diff(goal, ctx, recent_actions)
            if diff_batch and len(diff_batch.commands) > 0:
                print("üîÑ Attempting fallback with DOM diff...")
                diff_evidence = await self.browser.execute_action_batch(diff_batch)
                diff_evidence.fallback_used = True
                
                # If this worked, return it
                if diff_evidence.success or len(diff_evidence.errors) == 0:
                    print("‚úÖ DOM diff fallback succeeded!")
                    return diff_evidence
                    
                print(f"üîÑ DOM diff fallback also failed with {len(diff_evidence.errors)} errors.")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to create DOM diff fallback: {e}")
        
        # All fallbacks failed, but check if action succeeded despite API failure
        print("‚ùå All fallback attempts failed, checking for post-execution success...")
        evidence = await self._verify_action_success_despite_failure(evidence, goal, ctx, initial_batch)
        evidence.fallback_used = True
        return evidence

    async def _verify_action_success_despite_failure(
        self, 
        evidence: Evidence, 
        goal: str, 
        ctx: PageContext, 
        original_batch: CommandBatch
    ) -> Evidence:
        """
        Verify if an action succeeded despite API failures.
        
        Key insight: If the DOM changed significantly and contained noop commands 
        due to API failures, the action might have actually succeeded.
        """
        # Check if this was an API failure case (noop commands with DOM changes)
        has_noop_commands = any(cmd.type == "noop" for cmd in original_batch.commands)
        dom_changed = evidence.dom_after_sig != ctx.signature if hasattr(ctx, 'signature') else False
        
        if not (has_noop_commands and dom_changed):
            return evidence  # Not an API failure case
            
        print("üîç Potential API failure with DOM changes - verifying actual success...")
        
        try:
            # Get current page context to analyze if goal was achieved
            current_ctx = await self.browser.get_current_dom()
            
            # Create a verification prompt to check if the goal was achieved
            verification_prompt = f"""
You are analyzing whether a web automation goal was achieved despite an API connection failure.

**Goal:** {goal}

**Context:** The system experienced an API connection error while trying to execute an action, but the DOM changed significantly. This suggests the user might have manually completed the action or the page updated automatically.

**Current Page Content (key elements only):**
{current_ctx.ranked_elements[:20] if hasattr(current_ctx, 'ranked_elements') else 'Not available'}

**Instructions:**
Analyze the current page state and determine if the goal has been achieved. Look for:
1. Elements mentioned in the goal that are now present/selected
2. Evidence that the requested action was completed
3. Clear indicators of success (like items in cart, selections made, etc.)

Respond with JSON:
{{
    "achieved": true/false,
    "confidence": 0.0-1.0,
    "evidence": "specific evidence from the page that shows success or failure",
    "reasoning": "brief explanation of your analysis"
}}
"""
            
            # Query the LLM for verification (with retry logic)
            api_retry_count = 0
            max_retries = 2
            verification_response = None
            
            while api_retry_count < max_retries:
                try:
                    verification_response = self.llm.query_external_api(verification_prompt)
                    
                    # Check for connection errors
                    if ("I apologize, but I'm having trouble connecting" in verification_response or 
                        "Connection a" in verification_response):
                        api_retry_count += 1
                        if api_retry_count < max_retries:
                            await asyncio.sleep(1)
                            continue
                        else:
                            print("‚ö†Ô∏è Verification API also failed - keeping original failure status")
                            return evidence
                    break
                    
                except Exception as e:
                    api_retry_count += 1
                    if api_retry_count >= max_retries:
                        print(f"‚ö†Ô∏è Verification failed: {e}")
                        return evidence
                    await asyncio.sleep(1)
            
            # Parse verification response
            if verification_response:
                try:
                    json_start = verification_response.find('{')
                    json_end = verification_response.rfind('}') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_str = verification_response[json_start:json_end]
                        verification_data = json.loads(json_str)
                        
                        achieved = verification_data.get('achieved', False)
                        confidence = verification_data.get('confidence', 0.0)
                        verification_evidence = verification_data.get('evidence', '')
                        reasoning = verification_data.get('reasoning', '')
                        
                        if achieved and confidence > 0.7:
                            print(f"‚úÖ POST-VERIFICATION SUCCESS! Goal achieved despite API failure")
                            print(f"   Evidence: {verification_evidence}")
                            print(f"   Reasoning: {reasoning}")
                            
                            # Update evidence to reflect success
                            evidence.success = True
                            evidence.findings['post_verification'] = {
                                'achieved': True,
                                'confidence': confidence,
                                'evidence': verification_evidence,
                                'reasoning': reasoning,
                                'recovered_from_api_failure': True
                            }
                            # Clear API-related errors
                            evidence.errors = [e for e in evidence.errors if 'noop' not in e.lower()]
                        else:
                            print(f"‚ùå Post-verification confirmed failure (confidence: {confidence})")
                            
                except Exception as parse_error:
                    print(f"‚ö†Ô∏è Failed to parse verification response: {parse_error}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Post-execution verification failed: {e}")
            
        return evidence

    async def _create_fallback_batch_with_full_dom(
        self, 
        goal: str, 
        ctx: PageContext, 
        recent_actions: Optional[List[Dict[str, Any]]] = None
    ) -> Optional[CommandBatch]:
        """Create a new command batch using full DOM skeleton instead of ranked elements."""
        try:
            # Import dom_processor to get full skeleton
            from . import dom_processor as dp
            from utils.dom_skeleton import create_dom_skeleton
            
            # Get current DOM for full skeleton
            raw_dom, title = await self.browser.get_current_dom()
            compressed = dp.compress_dom(raw_dom, goal)
            full_skeleton = create_dom_skeleton(compressed)
            
            # Build a richer prompt with full DOM skeleton
            fallback_prompt = self._build_fallback_prompt_with_full_dom(goal, ctx, full_skeleton, recent_actions)
            
            # Query LLM with richer context
            raw_response = self.llm.query_external_api(fallback_prompt)
            
            # Debug: Save fallback prompt and response
            try:
                import os
                debug_dir = self.debug_run_folder if self.debug_run_folder else os.path.join(os.getcwd(), "debug_prompts")
                os.makedirs(debug_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                step_info = getattr(ctx, 'step_number', 'unknown')
                
                # Save fallback prompt
                prompt_file = os.path.join(debug_dir, f"fallback_prompt_step{step_info}_{timestamp}.txt")
                with open(prompt_file, 'w', encoding='utf-8') as f:
                    f.write("FALLBACK PROMPT WITH FULL DOM SKELETON\n")
                    f.write("=" * 50 + "\n")
                    f.write(fallback_prompt)
                
                # Save fallback response  
                response_file = os.path.join(debug_dir, f"fallback_response_step{step_info}_{timestamp}.txt")
                with open(response_file, 'w', encoding='utf-8') as f:
                    f.write("FALLBACK RESPONSE\n")
                    f.write("=" * 50 + "\n")
                    f.write(str(raw_response))
                
                print(f"üêõ DEBUG: Fallback prompt saved to {prompt_file}")
                print(f"üêõ DEBUG: Fallback response saved to {response_file}")
            except Exception as debug_error:
                print(f"‚ö†Ô∏è DEBUG: Failed to save fallback debug files: {debug_error}")
            
            commands, confidence, rationale, breadcrumb = self._parse_llm_json(raw_response)
            
            print(f"üîç FALLBACK DEBUG: Parsed {len(commands)} commands from LLM response")
            for i, cmd in enumerate(commands):
                print(f"  Command {i+1}: {cmd.type} selector='{cmd.selector}' text='{cmd.text}'")
            
            return CommandBatch(commands=commands)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error creating full DOM fallback: {e}")
            return None

    async def _create_fallback_batch_with_dom_diff(
        self, 
        goal: str, 
        ctx: PageContext, 
        recent_actions: Optional[List[Dict[str, Any]]] = None
    ) -> Optional[CommandBatch]:
        """Create a new command batch using DOM diff to show only what changed."""
        try:
            # This would require implementing DOM diff logic
            # For now, return None - we can implement this later if needed
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error creating DOM diff fallback: {e}")
            return None

    def _build_fallback_prompt_with_full_dom(
        self, 
        goal: str, 
        ctx: PageContext, 
        full_skeleton: str, 
        recent_actions: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """Build a prompt using the full DOM skeleton instead of ranked interactive elements."""
        recent_actions = recent_actions or []
        
        lines = []
        lines.append("System:")
        lines.append("FALLBACK MODE: The initial ranked elements failed to provide working selectors.")
        lines.append("You are a web-navigation expert. The full DOM skeleton is provided below.")
        lines.append("Analyze the ENTIRE skeleton and choose ANY selector that will advance the goal.")
        lines.append("You can use ANY element you see - buttons, divs, spans, inputs, links, etc.")
        lines.append("Return 1-3 JSON commands with the EXACT selectors you identify in the DOM.")
        lines.append("")
        
        lines.append(f"--- Goal ---")
        lines.append(goal.strip())
        lines.append("")
        
        lines.append(f"--- Page Info ---")
        lines.append(f"URL: {ctx.url}")
        lines.append(f"Title: {ctx.title}")
        lines.append(f"Current Step: {getattr(ctx, 'step_number', 'unknown')} of {getattr(ctx, 'total_steps', 'unknown')}")
        lines.append("")
        
        lines.append("--- FULL DOM SKELETON ---")
        lines.append("INSTRUCTIONS: Look through this ENTIRE skeleton for ANY element that could help achieve the goal.")
        lines.append("Focus on: input fields, buttons, clickable divs, forms, links, etc.")
        lines.append("")
        lines.append(full_skeleton)
        lines.append("")
        
        if recent_actions:
            lines.append("--- Recent Actions That Failed ---")
            for action in recent_actions[-3:]:  # Show last 3 failed actions
                lines.append(f"- {action}")
            lines.append("")
        
        lines.append("--- Required Response Format ---")
        lines.append("Return JSON with 'commands' array. Each command needs:")
        lines.append("- type: 'click', 'type', 'press', or 'navigate'")
        lines.append("- selector: CSS selector you found in the DOM above")
        lines.append("- text: (for type commands) what to type")
        lines.append("- key: (for press commands) key to press")
        lines.append("- url: (for navigate commands) URL to go to")
        lines.append("")
        lines.append("CRITICAL: Use EXACT selectors from the DOM skeleton above.")
        lines.append("Example: {'commands': [{'type': 'click', 'selector': '#specific-button-id'}]}")
        
        return "\n".join(lines)

    # -----------------
    # Helpers
    # -----------------
    def _parse_llm_json(self, obj: Any) -> Tuple[List[Command], float, str, str]:
        """Leniently coerce the LLM response to (commands, confidence, rationale, breadcrumb)."""
        # If the LLM returned a string, try json.loads
        if isinstance(obj, str):
            # First, try to extract JSON from markdown code blocks
            import re
            markdown_json_match = re.search(r'```json\s*(\{.*?\})\s*```', obj, re.DOTALL)
            if markdown_json_match:
                json_str = markdown_json_match.group(1)
                try:
                    obj = json.loads(json_str)
                except Exception:
                    obj = {}
            else:
                # Try direct JSON parsing
                try:
                    obj = json.loads(obj)
                except Exception:
                    obj = {}
        if not isinstance(obj, dict):
            obj = {}

        cmd_list = []
        for item in obj.get("commands", []) or []:
            if not isinstance(item, dict):
                continue
            t = (item.get("type") or "").strip().lower()
            
            # Handle press commands (for Enter key, etc.)
            if t == "press":
                key_value = item.get("key", "").strip()
                if key_value:
                    cmd = Command(
                        type="press",
                        key=key_value,
                        selector=None,
                        text=None,
                        url=None,
                        enter=None
                    )
                    cmd_list.append(cmd)
                continue
            
            # Check against actual ActionType values
            if t not in {"navigate", "wait_for", "click", "type", "select", "press"}:
                continue
                
            # Handle the 'key' parameter from LLM - map it to appropriate fields
            key_value = item.get("key")
            enter_value = None
            
            # If key is "Enter" or "Return", set enter=True for type commands
            if key_value and key_value.lower() in ["enter", "return"] and t == "type":
                enter_value = True
            
            cmd = Command(
                type=t,
                selector=item.get("selector"),
                text=item.get("text"),
                url=item.get("url"),
                enter=enter_value
            )
            cmd_list.append(cmd)
            
            # Auto-add Enter after type commands in search-like fields
            if t == "type" and self._looks_like_search_field(item):
                enter_cmd = Command(
                    type="press",
                    key="Enter",
                    selector=None,
                    text=None,
                    url=None,
                    enter=None
                )
                cmd_list.append(enter_cmd)

        # Default to a no-op if empty
        if not cmd_list:
            cmd_list = [Command(type="noop")]

        conf = obj.get("confidence")
        try:
            confidence = float(conf) if conf is not None else 0.5
        except Exception:
            confidence = 0.5

        rationale = obj.get("rationale") or ""
        breadcrumb = obj.get("breadcrumb") or ""
        return cmd_list[:3], confidence, rationale, breadcrumb

    def _looks_like_search_field(self, command_item: dict) -> bool:
        """Detect if a type command is targeting a search field that would benefit from auto-Enter."""
        selector = (command_item.get("selector", "") or "").lower()
        text = (command_item.get("text", "") or "").lower()
        
        # Check selector for search-related patterns
        search_patterns = [
            "search", "query", "q", "term", "location", "address", 
            "zip", "postal", "find", "lookup", "filter"
        ]
        
        # If selector contains search-related keywords
        for pattern in search_patterns:
            if pattern in selector:
                return True
        
        # Check if it's an input field (most common for search)
        if "input" in selector and any(attr in selector for attr in ["[type=", "[name=", "[id=", "[placeholder="]):
            return True
            
        # Check if the text being typed looks like a search term (short, no spaces suggesting forms)
        if text and len(text.strip()) < 50 and not any(char in text for char in ["@", "password", "email"]):
            return True
            
        return False

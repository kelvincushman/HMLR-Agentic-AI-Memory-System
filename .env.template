# ============================================================================
# HMLR CognitiveLattice Environment Configuration
# ============================================================================
# Copy this file to .env and uncomment the provider you want to use.
# Only ONE provider should be active at a time (uncomment one section).
#
# Fully Supported Providers: OpenAI, Google Gemini, xAI Grok, Anthropic Claude
# Coming Soon: DeepSeek, Local Models (via Ollama)
# ============================================================================

# ============================================================================
# OPTION 1: OpenAI (Default - Fully Supported ✅)
# ============================================================================
# Most popular, wide model selection, thinking models (o1/o3)
# Pricing: https://openai.com/api/pricing/

API_PROVIDER=openai
OPENAI_API_KEY=your-openai-api-key-here

# --- Model Configuration ---
HMLR_DEFAULT_MODEL=gpt-4.1-mini              # Fast, cheap workers ($0.15/1M tokens)
# HMLR_MAIN_MODEL=gpt-4.1-pro                # Optional: Premium user-facing ($2.50/1M)
# HMLR_MAIN_MODEL=o1-mini                    # Optional: Thinking model ($3/1M)

# --- Temperature ---
HMLR_DEFAULT_TEMPERATURE=0.1                 # Deterministic (0.0-2.0)
# HMLR_MAIN_TEMPERATURE=0.6                  # Optional: More natural for users

# --- Advanced (Optional) ---
# HMLR_MAIN_REASONING_EFFORT=medium          # For o1/o3: low, medium, high
# HMLR_DEFAULT_TOP_P=0.95                    # Nucleus sampling (0.0-1.0)


# ============================================================================
# OPTION 2: Google Gemini (Fully Supported ✅)
# ============================================================================
# Great thinking models, very affordable, fast
# Pricing: https://ai.google.dev/pricing
# Get key: https://aistudio.google.com/apikey

# API_PROVIDER=gemini
# GEMINI_API_KEY=your-gemini-api-key-here

# --- Model Configuration ---
# HMLR_DEFAULT_MODEL=gemini-1.5-flash        # Super fast/cheap ($0.075/1M)
# HMLR_MAIN_MODEL=gemini-2.0-flash-thinking-exp  # Thinking model (~$0.15/1M)

# --- Temperature ---
# HMLR_DEFAULT_TEMPERATURE=0.1
# HMLR_MAIN_TEMPERATURE=0.5

# --- Thinking Parameters ---
# HMLR_DEFAULT_REASONING_EFFORT=3            # Thinking budget 1-10 (higher = more)
# HMLR_MAIN_REASONING_EFFORT=8               # Deep thinking for users
# HMLR_DEFAULT_TOP_K=40                      # Top-K sampling


# ============================================================================
# OPTION 3: xAI Grok (Fully Supported ✅)
# ============================================================================
# X.ai's Grok models, competitive pricing
# Pricing: https://x.ai/api
# Get key: https://console.x.ai

# API_PROVIDER=grok
# XAI_API_KEY=your-xai-api-key-here

# --- Model Configuration ---
# HMLR_DEFAULT_MODEL=grok-2-latest           # Latest Grok model
# HMLR_MAIN_MODEL=grok-beta                  # Optional: Beta features

# --- Temperature ---
# HMLR_DEFAULT_TEMPERATURE=0.1
# HMLR_MAIN_TEMPERATURE=0.6


# ============================================================================
# OPTION 4: Anthropic Claude (Fully Supported ✅)
# ============================================================================
# Excellent for reasoning, long context (200K), safety-focused
# Pricing: https://www.anthropic.com/pricing
# Get key: https://console.anthropic.com

# API_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your-anthropic-key-here

# --- Model Configuration ---
# HMLR_DEFAULT_MODEL=claude-3-5-sonnet-20241022      # Best balance ($3/1M)
# HMLR_MAIN_MODEL=claude-3-7-sonnet-20250219        # Extended thinking ($3/1M)
# HMLR_DEFAULT_MODEL=claude-3-5-haiku-20241022      # Fast/cheap ($0.25/1M)

# --- Temperature ---
# HMLR_DEFAULT_TEMPERATURE=0.1
# HMLR_MAIN_TEMPERATURE=0.7


# ============================================================================
# OPTION 5: DeepSeek (Coming Soon - Not Yet Implemented ⚠️)
# ============================================================================
# Chinese model, very affordable, competitive quality
# Pricing: https://platform.deepseek.com/api-docs/pricing
# Get key: https://platform.deepseek.com
#
# TO USE: Requires adding DeepSeek support to external_api_client.py
# Compatible with OpenAI API format

# API_PROVIDER=deepseek                      # ⚠️ Not yet implemented
# DEEPSEEK_API_KEY=your-deepseek-key-here
# DEEPSEEK_API_BASE=https://api.deepseek.com/v1

# --- Model Configuration ---
# HMLR_DEFAULT_MODEL=deepseek-chat           # Standard chat model
# HMLR_MAIN_MODEL=deepseek-reasoner         # Reasoning model

# --- Temperature ---
# HMLR_DEFAULT_TEMPERATURE=0.1
# HMLR_MAIN_TEMPERATURE=0.6


# ============================================================================
# OPTION 6: Local Models via Ollama (Partially Supported )
# ============================================================================
# Run models locally on your machine - FREE, no API costs!
# Install: https://ollama.ai
# Models: https://ollama.ai/library
#
# Setup:
#   1. Install Ollama
#   2. Run: ollama pull llama3.2:3b
#   3. Run: ollama serve
#   4. Uncomment config below

# API_PROVIDER=openai                        # Use OpenAI-compatible API
# OPENAI_API_KEY=ollama                      # Dummy key (not used)
# OPENAI_API_BASE=http://localhost:11434/v1

# --- Model Configuration ---
# HMLR_DEFAULT_MODEL=llama3.2:3b             # Fast, 3B params
# HMLR_MAIN_MODEL=llama3.2:3b                # Can use same or different
# Popular models:
#   - llama3.2:3b (fast, good quality)
#   - mistral:7b (balanced)
#   - codellama:7b (code-focused)
#   - phi3:mini (very fast)

# --- Temperature ---
# HMLR_DEFAULT_TEMPERATURE=0.1


# ============================================================================
# TOKEN BUDGETS (Optional - Same for all providers)
# ============================================================================
# Adjust these if you need different token limits
# Defaults are sensible for most use cases

# CONTEXT_BUDGET_TOKENS=6000               # Total context budget
# MAX_RESPONSE_TOKENS=2000                 # Max response length
# FACT_EXTRACTION_MAX_TOKENS=500           # Fact extraction limit
# USER_PROFILE_MAX_TOKENS=300              # User profile summary
# HYDRATOR_MAX_TOKENS=50000                # Context retrieval (large)


# ============================================================================
# EMBEDDING CONFIGURATION (Optional - Same for all providers)
# ============================================================================
# Used for semantic search/RAG
# Default: bge-large-en-v1.5 (1024 dimensions)

# HMLR_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
# HMLR_EMBEDDING_DIM=1024

# Alternative: all-MiniLM-L6-v2 (384 dimensions, faster but lower quality)
# HMLR_EMBEDDING_MODEL=all-MiniLM-L6-v2
# HMLR_EMBEDDING_DIM=384


# ============================================================================
# DATABASE & OTHER SETTINGS (Optional)
# ============================================================================
# SLIDING_WINDOW_SIZE=20                   # Conversation history size
# COGNITIVE_LATTICE_DB=/path/to/custom.db  # Custom database location


# ============================================================================
# QUICK START EXAMPLES
# ============================================================================
#
#  Fastest Setup (OpenAI):
#   1. Set: API_PROVIDER=openai
#   2. Set: OPENAI_API_KEY=sk-...
#   3. Done! Uses gpt-4.1-mini by default
#
#  Budget Setup (Gemini):
#   1. Set: API_PROVIDER=gemini
#   2. Set: GEMINI_API_KEY=AIza...
#   3. Set: HMLR_DEFAULT_MODEL=gemini-3-flash-preview
#   4. Super cheap, very fast!
#
#  Thinking Setup (Gemini):
#   1. Set: API_PROVIDER=gemini
#   2. Set: GEMINI_API_KEY=AIza...
#   3. Set: HMLR_MAIN_MODEL=gemini-2.0-flash-thinking-exp
#   4. Set: HMLR_MAIN_REASONING_EFFORT=7
#   5. Workers stay fast, user gets deep thinking!
#
#   High Quality Setup (Claude):
#   1. Set: API_PROVIDER=anthropic
#   2. Set: ANTHROPIC_API_KEY=sk-ant-...
#   3. Set: HMLR_DEFAULT_MODEL=claude-3-5-sonnet-20241022
#   4. Excellent reasoning, 200K context!
#
#  Free Local Setup (Ollama):
#   1. Install Ollama from https://ollama.ai
#   2. Run: ollama pull llama3.2:3b
#   3. Run: ollama serve
#   4. Set: API_PROVIDER=openai
#   5. Set: OPENAI_API_BASE=http://localhost:11434/v1
#   6. Set: HMLR_DEFAULT_MODEL=llama3.2:3b
#   7. Zero API costs!
#
# ============================================================================


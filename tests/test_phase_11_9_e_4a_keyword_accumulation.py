"""
Phase 11.9.E: Test 4A - Keyword Accumulation

Tests gradual keyword accumulation as conversation evolves.
Verifies that the LLM extracts and accumulates keywords turn-by-turn.

This tests the METADATA EXTRACTION capability that the Governor relies on
for routing conversations to the correct bridge block.
"""

import pytest
import asyncio
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.external_api_client import ExternalAPIClient
from memory.metadata_extractor import MetadataExtractor


class TestKeywordAccumulation:
    """Test 4A: Gradual Keyword Accumulation via LLM Metadata Extraction."""
    
    def test_keyword_accumulation(self):
        """
        Scenario: 6-turn conversation about building a REST API.
        Expected: LLM extracts keywords turn-by-turn, keywords accumulate.
        
        This tests that the LLM can extract metadata (keywords, summary) from
        conversation turns, which the Governor uses for routing decisions.
        """
        print("\n" + "="*80)
        print("TEST 4A: Gradual Keyword Accumulation (LLM Metadata Extraction)")
        print("="*80)
        
        api_client = ExternalAPIClient()
        extractor = MetadataExtractor()
        
        # REST API conversation turns
        conversation_turns = [
            "I'm building a REST API",
            "Using Express.js and Node.js",
            "Need to add authentication with JWT",
            "MongoDB for data persistence",
            "Rate limiting to prevent abuse",
            "What about input validation?"
        ]
        
        print("\n[TURNS 1-6] Extracting metadata from each turn...")
        
        all_keywords = set()
        turn_metadata_list = []
        
        for i, user_message in enumerate(conversation_turns, 1):
            print(f"\n[TURN {i}]: \"{user_message}\"")
            
            # Build conversation history for context
            conversation_history = "\n".join([
                f"Turn {j}: {msg}" 
                for j, msg in enumerate(conversation_turns[:i], 1)
            ])
            
            # Prompt LLM to extract metadata from this turn
            metadata_prompt = f"""You are analyzing a conversation about building a REST API.

CONVERSATION SO FAR:
{conversation_history}

TASK: Extract metadata from Turn {i}.

Please provide:
1. Keywords from this turn (3-5 specific technical terms)
2. One-line summary of what this turn adds to the conversation

Format your response as:
==METADATA_START==
KEYWORDS: keyword1, keyword2, keyword3
SUMMARY: Brief summary of this turn
==METADATA_END==

IMPORTANT: Keywords should be specific technical terms (e.g., "Express.js", "JWT", "MongoDB"), not generic words."""
            
            # Get LLM response
            llm_response = api_client.query_external_api(metadata_prompt)
            
            # Extract metadata
            _, metadata = extractor.parse_response(llm_response)
            
            keywords = metadata.get('keywords', [])
            summary = metadata.get('summary', '')
            
            print(f"   Keywords extracted: {keywords}")
            print(f"   Summary: {summary}")
            
            # Accumulate keywords
            all_keywords.update(keywords)
            turn_metadata_list.append({
                'turn': i,
                'message': user_message,
                'keywords': keywords,
                'summary': summary
            })
        
        # === VALIDATION ===
        print("\n" + "="*80)
        print("VALIDATION: Keyword Accumulation")
        print("="*80)
        
        # [1/4] Verify keywords were extracted from each turn
        print("\n[1/4] Checking keyword extraction per turn...")
        
        turns_with_keywords = sum(1 for m in turn_metadata_list if len(m['keywords']) > 0)
        print(f"   Turns with keywords: {turns_with_keywords}/{len(conversation_turns)}")
        
        assert turns_with_keywords >= 4, \
            f"‚ùå FAIL: Only {turns_with_keywords} turns had keywords extracted!"
        
        print(f"   ‚úÖ PASS: {turns_with_keywords} turns extracted keywords")
        
        # [2/4] Verify keyword accumulation
        print("\n[2/4] Verifying cumulative keyword growth...")
        
        print(f"\n   Total unique keywords accumulated: {len(all_keywords)}")
        print(f"   Keywords: {sorted(list(all_keywords))}")
        
        assert len(all_keywords) >= 8, \
            f"‚ùå FAIL: Only {len(all_keywords)} unique keywords total!"
        
        print(f"   ‚úÖ PASS: {len(all_keywords)} unique keywords accumulated")
        
        # [3/4] Check for expected concepts
        print("\n[3/4] Checking for expected technical concepts...")
        
        # Expected concepts from the conversation
        expected_concepts = [
            "REST", "API", "Express", "Node", "JWT", 
            "MongoDB", "authentication", "database", 
            "rate", "limiting", "validation"
        ]
        
        all_keywords_lower = {kw.lower() for kw in all_keywords}
        found_concepts = []
        
        for concept in expected_concepts:
            # Check if concept appears in any keyword (partial match)
            if any(concept.lower() in kw for kw in all_keywords_lower):
                found_concepts.append(concept)
        
        coverage = len(found_concepts) / len(expected_concepts)
        
        print(f"\n   Expected concepts: {expected_concepts}")
        print(f"   Found in keywords: {found_concepts}")
        print(f"   Coverage: {coverage:.1%}")
        
        assert coverage >= 0.4, \
            f"‚ùå FAIL: Only {coverage:.1%} concept coverage!"
        
        print(f"   ‚úÖ PASS: {coverage:.1%} of expected concepts found")
        
        # [4/4] Test cumulative metadata extraction
        print("\n[4/4] Testing cumulative metadata (all 6 turns)...")
        
        # Ask LLM to extract overall metadata from the full conversation
        full_conversation = "\n".join([
            f"Turn {i}: {msg}" 
            for i, msg in enumerate(conversation_turns, 1)
        ])
        
        cumulative_prompt = f"""You are analyzing a complete 6-turn conversation about building a REST API.

FULL CONVERSATION:
{full_conversation}

TASK: Extract cumulative metadata from the entire conversation.

Please provide:
1. All key technical topics discussed (aim for 10-15 keywords)
2. Overall summary of what the conversation covered

Format your response as:
==METADATA_START==
KEYWORDS: keyword1, keyword2, keyword3, etc.
SUMMARY: Brief overall summary
==METADATA_END==

IMPORTANT: Keywords should capture all major technical concepts discussed."""
        
        llm_response = api_client.query_external_api(cumulative_prompt)
        _, cumulative_metadata = extractor.parse_response(llm_response)
        
        cumulative_keywords = cumulative_metadata.get('keywords', [])
        cumulative_summary = cumulative_metadata.get('summary', '')
        
        print(f"\n   Cumulative keywords ({len(cumulative_keywords)}): {cumulative_keywords}")
        print(f"   Cumulative summary: {cumulative_summary}")
        
        assert len(cumulative_keywords) >= 8, \
            f"‚ùå FAIL: Cumulative extraction only got {len(cumulative_keywords)} keywords!"
        
        print(f"   ‚úÖ PASS: Cumulative extraction got {len(cumulative_keywords)} keywords")
        
        # === GOVERNOR ROUTING SIMULATION ===
        print("\n" + "="*80)
        print("GOVERNOR ROUTING SIMULATION")
        print("="*80)
        
        print("\nIn production, the Governor would:")
        print(f"   1. Store these {len(cumulative_keywords)} keywords in bridge_blocks.keywords")
        print("   2. Store this summary in bridge_blocks.summary")
        print("   3. Use keywords to match future queries to this block")
        
        # Simulate routing decision
        print("\nüß™ SIMULATION: New query arrives...")
        test_queries = [
            ("How do I set up JWT authentication?", True),  # Should match
            ("What's the weather like?", False)  # Should NOT match
        ]
        
        for query, should_match in test_queries:
            print(f"\n   Query: \"{query}\"")
            
            # Use ALL accumulated keywords (not just cumulative extraction)
            # This is more realistic - Governor would have all turn keywords
            all_keyword_text = " ".join(all_keywords).lower()
            query_lower = query.lower()
            
            # Check if query words appear in keyword corpus
            has_match = any(word in all_keyword_text for word in query_lower.split() if len(word) > 2)
            
            print(f"   Matching against {len(all_keywords)} accumulated keywords")
            print(f"   Would route to this block: {has_match}")
            
            if should_match:
                # For "JWT authentication" query, check if relevant keywords present
                tech_present = any(kw in all_keyword_text for kw in ['jwt', 'auth', 'token'])
                assert tech_present, f"‚ùå FAIL: Should have JWT/auth keywords but didn't!"
                print(f"   ‚úÖ Correct: Query would match REST API block (found auth/JWT keywords)")
            else:
                # Weather query shouldn't match REST API keywords
                weather_match = 'weather' in all_keyword_text
                assert not weather_match, f"‚ùå FAIL: Shouldn't match weather query!"
                print(f"   ‚úÖ Correct: Query would NOT match REST API block")
        
        # === FINAL SUMMARY ===
        print("\n" + "="*80)
        print("TEST 4A RESULTS")
        print("="*80)
        
        print("\n‚úÖ ALL VALIDATIONS PASSED:")
        print(f"   [‚úÖ] Turn-by-turn metadata extraction working")
        print(f"   [‚úÖ] Keywords accumulated: {len(all_keywords)} unique")
        print(f"   [‚úÖ] Cumulative extraction: {len(cumulative_keywords)} keywords")
        print(f"   [‚úÖ] Expected concepts coverage: {coverage:.1%}")
        print(f"   [‚úÖ] Governor routing simulation successful")
        
        print("\nüìä METADATA EXTRACTED:")
        for metadata in turn_metadata_list:
            print(f"   Turn {metadata['turn']}: {len(metadata['keywords'])} keywords - {metadata['keywords']}")
        
        print(f"\n   Cumulative: {len(cumulative_keywords)} keywords - {cumulative_keywords}")
        
        print("\nüéØ KEY ACHIEVEMENT:")
        print("   This test proves that:")
        print("   1. LLM can extract keywords from conversation turns")
        print("   2. Keywords accumulate as conversation progresses")
        print("   3. Cumulative extraction captures all major concepts")
        print("   4. Governor can use keywords for routing decisions")
        print("   5. Metadata grows richer with each turn")
        
        print("\nüèÜ ARCHITECTURAL WIN:")
        print("   ‚úÖ Metadata extraction works (LLM ‚Üí keywords)")
        print("   ‚úÖ Keywords accumulate (not replaced)")
        print("   ‚úÖ Governor has data for smart routing")
        print("   ‚úÖ Bridge block metadata evolves with conversation")
        
        print("\n" + "="*80)
        print("TEST 4A (Keyword Accumulation): ‚úÖ PASSED")
        print("="*80)


if __name__ == "__main__":
    """
    Run test individually for debugging:
    
    python -m pytest tests/test_phase_11_9_e_4a_keyword_accumulation.py::TestKeywordAccumulation::test_keyword_accumulation -v -s
    """
    pytest.main([__file__, "-v", "-s"])



